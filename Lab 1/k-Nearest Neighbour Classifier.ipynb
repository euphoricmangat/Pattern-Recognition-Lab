{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbour Classifier\n",
    "***\n",
    "It is strange to only use the label of the nearest image when we wish to make a prediction. Indeed, it is almost always the case that one can do better by using what’s called a k-Nearest Neighbor Classifier. The idea is very simple: instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image. In particular, when k = 1, we recover the Nearest Neighbor classifier. Intuitively, higher values of k have a smoothing effect that makes the classifier more resistant to outliers:\n",
    "![figure 1](./images/boundaries.png)\n",
    "\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. A useful technique can be to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor. A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance).\n",
    "\n",
    "## Validation sets for Hyperparameter tuning\n",
    "***\n",
    "The k-nearest neighbor classifier requires a setting for k. But what number works best? Additionally, we saw that there are many different distance functions we could have used: L1 norm, L2 norm, there are many other choices we didn’t even consider (e.g. dot products). These choices are called **hyperparameters** and they come up very often in the design of many Machine Learning algorithms that learn from data. It’s often not obvious what values/settings one should choose.\n",
    "\n",
    "We might be tempted to suggest that we should try out many different values and see what works best. But, doing this will result in _overfitting_ the testcase. **We cannot use the test set for the purpose of tweaking hyperparameters.** There is a correct way of tuning the hyperparameters and it does not touch the test set at all. The idea is to split our training set in two: a slightly smaller training set, and what we call a _validation set_.\n",
    "***\n",
    "\n",
    "### Problem 1: Classifying Points\n",
    "Given some data points (in 2D space) with their color labels, design a kNN classifier to assign a color label to the test point.\n",
    "\n",
    "Given data points:\n",
    "\n",
    "|Sno| Coordinates| Color|\n",
    "|:---:| :----------: | :----: |\n",
    "|1. | (1, 1)     | red  |\n",
    "|2. | (2, 2.5)   | red  |\n",
    "|3. | (3, 1.2)   | red  |\n",
    "|4. | (5.5, 6.3) | blue |\n",
    "|5. | (6, 9)     | blue |\n",
    "|6. | (7, 6)     | blue |\n",
    "\n",
    "Test data:\n",
    "\n",
    "|Sno| Coordinates| Color| True Value |\n",
    "|:---:| :----------: | :----: |:----:|\n",
    "|1.   | (3, 4)       | ?      | red|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEZCAYAAAB/6SUgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe784026c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given test point belongs to class with color: red\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define training data\n",
    "training_data = np.array([[1,1], [2,2.5], [3,1.2], [5.5,6.3], [6,9], [7, 6]])\n",
    "training_labels = ['red', 'red', 'red', 'blue', 'blue', 'blue']\n",
    "\n",
    "# define test data\n",
    "test_data = [3, 4]\n",
    "\n",
    "# Plot the training and test points\n",
    "plt.figure()\n",
    "plt.scatter(training_data[:, 0], training_data[:, 1], color = training_labels[:], s = 170)\n",
    "plt.scatter(test_data[0], test_data[1], color = 'green', s = 170)\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Data Points')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# define distance function\n",
    "def euclidian_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "# process of calculating the distance of test instance from all items of training set\n",
    "# function that returns k nearest neighbors from the training set for a given test instance\n",
    "def getNeighbours(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    for i in range(len(trainingSet)):\n",
    "        dist = euclidian_distance(trainingSet[i], testInstance)\n",
    "        distances.append((trainingSet[i], training_labels[i], dist))\n",
    "    \n",
    "    # sort in accending order w.r.t distance\n",
    "    distances.sort(key = operator.itemgetter(2))\n",
    "    neighbours = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        neighbours.append((distances[i][1], 1/distances[i][2]))\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "# Once we have located the most similar neighbors for a test instance, \n",
    "# the next task is to devise a predicted response based on those neighbors.\n",
    "# We can do this by allowing each neighbor to vote for their class attribute,\n",
    "# and take the majority vote as the prediction.\n",
    "# value of each vote depends upon its weight (calculated as 1/distance)\n",
    "# Below provides a function for getting the majority voted response from a number of neighbors.       \n",
    "def getResponse(neighbours):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbours)):\n",
    "        response = neighbours[x][0]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1 * neighbours[x][1]\n",
    "        else:\n",
    "            classVotes[response] = 1 * neighbours[x][1]\n",
    "    # sort the votes in decending order and take the label with maxm votes\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "# using k = 3\n",
    "neighbours = getNeighbours(training_data, test_data, 3)\n",
    "print('The given test point belongs to class with color:', getResponse(neighbours))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Classifying Images:\n",
    "***\n",
    "Design a kNN classifier to correctly identify digits from a dataset of tens of thousands of handwritten images.\n",
    "Define two cases with different number of data values for training the model and output the accuracy of the model in each case.\n",
    "\n",
    "**Given**: Use the [Digit dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html) : Sklearn is a Python modules for machine learning. This module comes with a few standard datasets, such as the digits dataset. This dataset is made up of 1797 8x8 images and contains two arrays _digits.images_ and _digits.target_.\n",
    "\n",
    "Case 1: 100 data vales as training data <br>\n",
    "Case 2: 1000 data values as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Case 1: 83.0\n",
      "Accuracy for Case 2: 90.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Define the distance function:\n",
    "def euclidian_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "# Define the accuracy function:\n",
    "def getAccuracy(testInstance, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testInstance)):\n",
    "        if str(testInstance[i]) == predictions[i]:\n",
    "            correct += 1\n",
    "    return (correct / float(len(testInstance))) * 100.0\n",
    "\n",
    "# process of calculating the distance of test instance from all items of training set\n",
    "# function that returns k nearest neighbors from the training set for a given test instance\n",
    "def getNeighbours(training_data, training_labels, testInstance, k):\n",
    "    distances = []\n",
    "    for i in range(len(training_data)):\n",
    "        dist = euclidian_distance(training_data[i], testInstance)\n",
    "        distances.append((training_data[i], training_labels[i], dist))\n",
    "    \n",
    "    # sort in accending order w.r.t distance\n",
    "    distances.sort(key = operator.itemgetter(2))\n",
    "    neighbours = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        neighbours.append((distances[i][1], 1/distances[i][2]))\n",
    "    return neighbours\n",
    "\n",
    "# Once we have located the most similar neighbors for a test instance, \n",
    "# the next task is to devise a predicted response based on those neighbors.\n",
    "# We can do this by allowing each neighbor to vote for their class attribute,\n",
    "# and take the majority vote as the prediction.\n",
    "# value of each vote depends upon its weight (calculated as 1/distance)\n",
    "# Below provides a function for getting the majority voted response from a number of neighbors.       \n",
    "def getResponse(neighbours):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbours)):\n",
    "        response = str(neighbours[x][0])\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1 * neighbours[x][1]\n",
    "        else:\n",
    "            classVotes[response] = 1 * neighbours[x][1]\n",
    "    # sort the votes in decending order and take the label with maxm votes\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "# use fist 100 images as training data for case 1 and first 1000 images for case 2\n",
    "training_data1 = digits.data[0:100]\n",
    "training_labels1 = digits.target[0:100]\n",
    "training_data2 = digits.data[0:1000]\n",
    "training_labels2 = digits.target[0:1000]\n",
    "\n",
    "\n",
    "# use random 100 values as test data\n",
    "test_data = digits.data[1597:1697]\n",
    "test_labels = digits.target[1597:1697]\n",
    "\n",
    "# using k = 5\n",
    "predictions1 = []\n",
    "predictions2 = []\n",
    "for i in range(len(test_data)):\n",
    "    neighbours1 = getNeighbours(training_data1, training_labels1, test_data[i], 5)\n",
    "    neighbours2 = getNeighbours(training_data2, training_labels2, test_data[i], 5)\n",
    "    predictions1.append(getResponse(neighbours1))\n",
    "    predictions2.append(getResponse(neighbours2))\n",
    "    \n",
    "print('Accuracy for Case 1:', getAccuracy(test_labels, predictions1))\n",
    "print('Accuracy for Case 2:', getAccuracy(test_labels, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Classification of Iris Dataset:\n",
    "***\n",
    "Fisher’s Iris data base (Fisher, 1936) is perhaps the best known\n",
    "database to be found in the pattern recognition literature. The data\n",
    "set contains 3 classes of 50 instances each, where each class refers\n",
    "to a type of iris plant. One class is linearly separable from the other\n",
    "two; the latter are not linearly separable from each other.\n",
    "The data base contains the following attributes:<br>\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class:\n",
    "    - Iris Setosa\n",
    "    - Iris Versicolour\n",
    "    - Iris Virginica Design\n",
    "    \n",
    "Design a kNN classifier to classify the given test spicies as belonging to one of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 103\n",
      "Test set: 47\n",
      "1. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "2. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "3. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "4. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "5. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "6. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "7. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "8. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "9. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "10. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "11. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "12. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "13. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "14. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "15. predicted='Iris-setosa', actual='Iris-setosa'\n",
      "16. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "17. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "18. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "19. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "20. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "21. predicted='Iris-virginica', actual='Iris-versicolor'\n",
      "22. predicted='Iris-virginica', actual='Iris-versicolor'\n",
      "23. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "24. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "25. predicted='Iris-virginica', actual='Iris-versicolor'\n",
      "26. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "27. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "28. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "29. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "30. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "31. predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "32. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "33. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "34. predicted='Iris-versicolor', actual='Iris-virginica'\n",
      "35. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "36. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "37. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "38. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "39. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "40. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "41. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "42. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "43. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "44. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "45. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "46. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "47. predicted='Iris-virginica', actual='Iris-virginica'\n",
      "Accuracy: 91.48936170212765%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import operator\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Load the IRIS dataset csv file and convert the data into a list of lists (2D array),\n",
    "# make sure the data file is in the current working directory\n",
    "# Randomly split dataset to training and testing data\n",
    "def loadDataset(filename, split, training_set=[], test_set=[]):\n",
    "    with open(filename, 'rt') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "        \n",
    "        for i in range(len(dataset)-1):\n",
    "            for j in range(4):\n",
    "                dataset[i][j] = float(dataset[i][j])\n",
    "                \n",
    "            if random.random() < split:\n",
    "                training_set.append(dataset[i])\n",
    "            else:\n",
    "                test_set.append(dataset[i])\n",
    "            \n",
    "# function to calculate euclidian distance between two point;\n",
    "# this distance gives a measure of similarity between the two\n",
    "def euclidianDistance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "# process of calculating the distance of test instance from all items of training set\n",
    "# function that returns k nearest neighbors from the training set for a given test instance\n",
    "def getNeighbours(training_set, test_instance, k):\n",
    "    distance = []\n",
    "    for i in range(len(training_set)):\n",
    "        dist = euclidianDistance(np.array(training_set[i][:-1]), np.array(test_instance[:-1]))\n",
    "        if dist == 0:\n",
    "            dist = 0.000000009\n",
    "        distance.append((training_set[i], dist))\n",
    "    \n",
    "    distance.sort(key=operator.itemgetter(1))\n",
    "    neighbours = []\n",
    "    for i in range(len(distance)):\n",
    "        neighbours.append((distance[i][0], 1/distance[i][1]))\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "# Once we have located the most similar neighbors for a test instance, \n",
    "# the next task is to devise a predicted response based on those neighbors.\n",
    "# We can do this by allowing each neighbor to vote for their class attribute,\n",
    "# and take the majority vote as the prediction.\n",
    "# value of each vote depends upon its weight (calculated as 1/distance)\n",
    "# Below provides a function for getting the majority voted response from a number of neighbors.       \n",
    "def getResponse(neighbours):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbours)):\n",
    "        response = neighbours[x][0][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1 * neighbours[x][1]\n",
    "        else:\n",
    "            classVotes[response] = 1 * neighbours[x][1]\n",
    "    # sort the votes in decending order and take the label with maxm votes\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "# Define the accuracy function:\n",
    "def getAccuracy(test_set, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i][-1] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (correct / len(test_set)) * 100.0\n",
    "\n",
    "def main():\n",
    "    # prepare data\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    # 2/3 for training and 1/3 for testing\n",
    "    split = 0.67\n",
    "    loadDataset('iris.data.csv', split, training_set, test_set)\n",
    "    print('Train set: ' + repr(len(training_set)))\n",
    "    print('Test set: ' + repr(len(test_set)))\n",
    "    # generate predictions\n",
    "    predictions = []\n",
    "    k = 3\n",
    "    for x in range(len(test_set)):\n",
    "        neighbours = getNeighbours(training_set, test_set[x], k)\n",
    "        result = getResponse(neighbours)\n",
    "        predictions.append(result)\n",
    "        print('{}. predicted='.format(x+1) + repr(result) + ', actual=' + repr(test_set[x][-1]))\n",
    "    accuracy = getAccuracy(test_set, predictions)\n",
    "    print('Accuracy: ' + repr(accuracy) + '%')\n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros of kNN Classifier:\n",
    "***\n",
    "- It is very simple to implement and understand\n",
    "- The classifier takes no time to train, since all that is required is to store and possibly index the training data\n",
    "- Naturally handles multi-class cases. (_KNN works just as easily with multiclass data sets whereas other algorithms are hardcoded for the binary setting._)\n",
    "- New training examples can be added easily. \n",
    "- The non-parametric nature of KNN gives it an edge in certain settings where the data may be highly “unusual”.\n",
    "\n",
    "## Cons of kNN Classifier:\n",
    "***\n",
    "- Comparitively higher computational cost since classifying a test example requires a comparison to every single training example.\n",
    "- Not appropriate for high-dimensional objects since distances over high-dimensional spaces can be very counter-intuitive (_The accuracy of KNN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbors._)\n",
    "- KNN can suffer from skewed class distributions. For example, if a certain class is very frequent in the training set, it will tend to dominate the majority voting of the new example (large number = more common). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
